{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba45051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.13-cp39-cp39-win_amd64.whl (164 kB)\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.13\n"
     ]
    }
   ],
   "source": [
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46aaa911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wave\n",
      "  Downloading Wave-0.0.2.zip (38 kB)\n",
      "Building wheels for collected packages: wave\n",
      "  Building wheel for wave (setup.py): started\n",
      "  Building wheel for wave (setup.py): finished with status 'done'\n",
      "  Created wheel for wave: filename=Wave-0.0.2-py3-none-any.whl size=1220 sha256=a3e022fec820c79ae36ccf1796cedd1b81077a2868f9e13b3f8b74b0e9677fa8\n",
      "  Stored in directory: c:\\users\\shweta\\appdata\\local\\pip\\cache\\wheels\\ee\\3e\\b3\\bdee8d885ec04fd42bd239e9d9ab9dc94e69aad318c0d9936c\n",
      "Successfully built wave\n",
      "Installing collected packages: wave\n",
      "Successfully installed wave-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d678dc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done recording\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024 \n",
    "FORMAT = pyaudio.paInt16 #paInt8\n",
    "CHANNELS = 2 \n",
    "RATE = 44100 #sample rate\n",
    "RECORD_SECONDS = 4\n",
    "WAVE_OUTPUT_FILENAME = \"output10.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK) #buffer\n",
    "\n",
    "print(\"* recording\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data) # 2 bytes(16 bits) per channel\n",
    "\n",
    "print(\"* done recording\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cab4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b17cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from librosa) (1.7.3)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from librosa) (1.21.5)\n",
      "Collecting pooch<1.7,>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from librosa) (4.1.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from librosa) (1.0.2)\n",
      "Collecting soxr>=0.3.2\n",
      "  Downloading soxr-0.3.5-cp39-cp39-win_amd64.whl (184 kB)\n",
      "Collecting soundfile>=0.12.1\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "Collecting lazy-loader>=0.1\n",
      "  Downloading lazy_loader-0.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from librosa) (0.55.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (67.6.1)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from pooch<1.7,>=1.0->librosa) (21.3)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from pooch<1.7,>=1.0->librosa) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from packaging>=20.0->pooch<1.7,>=1.0->librosa) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Building wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py): started\n",
      "  Building wheel for audioread (setup.py): finished with status 'done'\n",
      "  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23703 sha256=6e841419b81e24c4a4d079732d0fdbbcebc214babd8478e718152a5465209902\n",
      "  Stored in directory: c:\\users\\shweta\\appdata\\local\\pip\\cache\\wheels\\e4\\76\\a4\\cfb55573167a1f5bde7d7a348e95e509c64b2c3e8f921932c3\n",
      "Successfully built audioread\n",
      "Installing collected packages: soxr, soundfile, pooch, lazy-loader, audioread, librosa\n",
      "Successfully installed audioread-3.0.0 lazy-loader-0.2 librosa-0.10.0.post2 pooch-1.6.0 soundfile-0.12.1 soxr-0.3.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffce2dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pad_sequences\n",
      "  Downloading pad-sequences-0.6.1.tar.gz (9.5 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement from (from versions: none)\n",
      "ERROR: No matching distribution found for from\n"
     ]
    }
   ],
   "source": [
    "pip install pad_sequences from keras.preprocessing.sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b17df54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8310998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71681f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51d47226",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist= os.listdir('C:/Users/Shweta/Downloads/NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bc023b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63942907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'03-01-02-02-01-01-09.wav'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91aac4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\shweta\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from soundfile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shweta\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c91c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# all emotions on RAVDESS dataset\n",
    "int2emotion = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "# we allow only these emotions\n",
    "AVAILABLE_EMOTIONS = {\n",
    "    \"angry\",\n",
    "    \"sad\",\n",
    "    \"neutral\",\n",
    "    \"happy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34e08ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma or contrast:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "        if contrast:\n",
    "            contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, contrast))\n",
    "        if tonnetz:\n",
    "            tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, tonnetz))\n",
    "    return result\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "454e4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size=0.2):\n",
    "    X, y = [], []\n",
    "    try :\n",
    "      for file in glob.glob(\"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"):\n",
    "          # get the base name of the audio file\n",
    "          basename = os.path.basename(file)\n",
    "          print(basename)\n",
    "          # get the emotion label\n",
    "          emotion = int2emotion[basename.split(\"-\")[2]]\n",
    "          # we allow only AVAILABLE_EMOTIONS we set\n",
    "          if emotion not in AVAILABLE_EMOTIONS:\n",
    "              continue\n",
    "          # extract speech features\n",
    "          features = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "          # add to data\n",
    "          X.append(features)\n",
    "          l={'happy':0.0,'sad':1.0,'neutral':3.0,'angry':4.0}\n",
    "          y.append(l[emotion])\n",
    "    except :\n",
    "         pass\n",
    "    # split the data to training and testing and return it\n",
    "    return train_test_split(np.array(X), y, test_size=test_size, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a32b98db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[+] Number of training samples:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# number of samples in testing data\u001b[39;00m\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(test_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m      \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# split the data to training and testing and return it\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_data(test_size=0.25)\n",
    "\n",
    "print(\"[+] Number of training samples:\", X_train.shape[0])\n",
    "# number of samples in testing data\n",
    "print(\"[+] Number of testing samples:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa0bd2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46bea4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ddccb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from each audio file\n",
    "def extract_features(file_path):\n",
    "    audio, sampling_rate = librosa.load(file_path, sr=22050, duration=None)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=30)\n",
    "    features = np.mean(mfccs.T, axis=0)\n",
    "    return features\n",
    "\n",
    "# Function to extract features from each audio file without silent moments\n",
    "def extract_features_trim(file_path):\n",
    "    audio, sampling_rate = librosa.load(file_path, sr=22050, duration=None)\n",
    "    audio, index = librosa.effects.trim(audio, top_db=20, frame_length=512, hop_length=256)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=30)\n",
    "    features = np.mean(mfccs.T, axis=0)\n",
    "    return features\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(dataset_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        for file in os.listdir(os.path.join(dataset_path, folder)):\n",
    "            if file.endswith('.wav'):\n",
    "                \n",
    "                emotion = file.split('-')[2]\n",
    "                if int(emotion) == 1:\n",
    "                    label = 'Neutral'\n",
    "                elif int(emotion) == 2:\n",
    "                    label = 'Calm'\n",
    "                elif int(emotion) == 3:\n",
    "                    label = 'Happy'\n",
    "                elif int(emotion) == 4:\n",
    "                    label = 'Sad'\n",
    "                elif int(emotion) == 5:\n",
    "                    label = 'Angry'\n",
    "                elif int(emotion) == 6:\n",
    "                    label = 'Fearful'\n",
    "                elif int(emotion) == 7:\n",
    "                    label = 'Disgust'\n",
    "                elif int(emotion) == 8:\n",
    "                    label = 'Surprised'\n",
    "                else:\n",
    "                    label = 'UNK'\n",
    "                    \n",
    "                file_path = os.path.join(dataset_path, folder, file)\n",
    "                features = extract_features(file_path)\n",
    "                X.append(features)\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function for displaying confusion matrix plot\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix For {}'.format(title))\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e2d855c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: 'https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print the shape of the feature matrix and the label array\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of X:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(dataset_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, folder)):\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio'"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X, y = preprocess_data(dataset_path)\n",
    "\n",
    "# Print the shape of the feature matrix and the label array\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f94ba22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Shweta'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fd47104",
   "metadata": {},
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f94fbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.arduinoIDE',\n",
       " '.cache',\n",
       " '.conda',\n",
       " '.condarc',\n",
       " '.continuum',\n",
       " '.eclipse',\n",
       " '.idlerc',\n",
       " '.ipynb_checkpoints',\n",
       " '.ipython',\n",
       " '.jupyter',\n",
       " '.keras',\n",
       " '.matplotlib',\n",
       " '.opera',\n",
       " '.p2',\n",
       " '.spyder-py3',\n",
       " '.tooling',\n",
       " '.VirtualBox',\n",
       " '.vscode',\n",
       " '.Xilinx',\n",
       " '3D Objects',\n",
       " 'ADA1.ipynb',\n",
       " 'ADA2.ipynb',\n",
       " 'AdaClassificationlab.ipynb',\n",
       " 'Adalab.csv',\n",
       " 'ADAlab_Project-Copy1.ipynb',\n",
       " 'ADAlab_Project.ipynb',\n",
       " 'all-data.csv',\n",
       " 'anaconda3',\n",
       " 'AppData',\n",
       " 'Application Data',\n",
       " 'Assignment-2 nd 4 NLP.ipynb',\n",
       " 'auction.ipynb',\n",
       " 'awerdt45',\n",
       " 'BullseyeCoverageError.txt',\n",
       " 'car_evaluation.csv',\n",
       " 'catboost_info',\n",
       " 'Contacts',\n",
       " 'Cookies',\n",
       " 'Corona_NLP_test.csv',\n",
       " 'Customer_Segmentation.ipynb',\n",
       " 'Desktop',\n",
       " 'Documents',\n",
       " 'Downloads',\n",
       " 'eclipse',\n",
       " 'eclipse-workspace',\n",
       " 'EcoPreprocessed.csv',\n",
       " 'Favorites',\n",
       " 'FinalAdalab - anemia (1).csv',\n",
       " 'IntelGraphicsProfiles',\n",
       " 'Jedi',\n",
       " 'LAB 2.ipynb',\n",
       " 'Lab-1.v',\n",
       " 'Links',\n",
       " 'Litdac.csv',\n",
       " 'Local Settings',\n",
       " 'Mall_Customers.csv',\n",
       " 'ml project.ipynb',\n",
       " 'MLProject_L3-4.ipynb',\n",
       " 'ML_OnlineFraud_L3-4.ipynb',\n",
       " 'Music',\n",
       " 'My Documents',\n",
       " 'NetHood',\n",
       " 'NLP LAB2.ipynb',\n",
       " 'NLP Project.ipynb',\n",
       " 'NLPLab1.ipynb',\n",
       " 'NTUSER.DAT',\n",
       " 'ntuser.dat.LOG1',\n",
       " 'ntuser.dat.LOG2',\n",
       " 'NTUSER.DAT{53b39e87-18c4-11ea-a811-000d3aa4692b}.TxR.0.regtrans-ms',\n",
       " 'NTUSER.DAT{53b39e87-18c4-11ea-a811-000d3aa4692b}.TxR.1.regtrans-ms',\n",
       " 'NTUSER.DAT{53b39e87-18c4-11ea-a811-000d3aa4692b}.TxR.2.regtrans-ms',\n",
       " 'NTUSER.DAT{53b39e87-18c4-11ea-a811-000d3aa4692b}.TxR.blf',\n",
       " 'NTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TM.blf',\n",
       " 'NTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TMContainer00000000000000000001.regtrans-ms',\n",
       " 'NTUSER.DAT{53b39e88-18c4-11ea-a811-000d3aa4692b}.TMContainer00000000000000000002.regtrans-ms',\n",
       " 'ntuser.ini',\n",
       " 'OneDrive',\n",
       " 'onlinefraud.csv',\n",
       " 'output10.wav',\n",
       " 'Participant.csv',\n",
       " 'phones_data.csv',\n",
       " 'Pictures',\n",
       " 'Planet.xlsx',\n",
       " 'Planets.xlsx',\n",
       " 'PrintHood',\n",
       " 'prog.ipynb',\n",
       " 'project_1',\n",
       " 'project_2',\n",
       " 'PycharmProjects',\n",
       " 'Recent',\n",
       " 'Sample.txt',\n",
       " 'Saved Games',\n",
       " 'Searches',\n",
       " 'SendTo',\n",
       " 'softcomputingproject.ipynb',\n",
       " 'Start Menu',\n",
       " 'Templates',\n",
       " 'tennis.csv',\n",
       " 'test.csv',\n",
       " 'Tracing',\n",
       " 'twitter_training.csv',\n",
       " 'Untitled.ipynb',\n",
       " 'Untitled1.ipynb',\n",
       " 'Untitled10.ipynb',\n",
       " 'Untitled11.ipynb',\n",
       " 'Untitled12.ipynb',\n",
       " 'Untitled13.ipynb',\n",
       " 'Untitled14.ipynb',\n",
       " 'Untitled15.ipynb',\n",
       " 'Untitled16.ipynb',\n",
       " 'Untitled17.ipynb',\n",
       " 'Untitled18.ipynb',\n",
       " 'Untitled19.ipynb',\n",
       " 'Untitled2.ipynb',\n",
       " 'Untitled20.ipynb',\n",
       " 'Untitled21.ipynb',\n",
       " 'Untitled22.ipynb',\n",
       " 'Untitled23.ipynb',\n",
       " 'Untitled24.ipynb',\n",
       " 'Untitled25.ipynb',\n",
       " 'Untitled26.ipynb',\n",
       " 'Untitled27.ipynb',\n",
       " 'Untitled28.ipynb',\n",
       " 'Untitled29.ipynb',\n",
       " 'Untitled3.ipynb',\n",
       " 'Untitled30.ipynb',\n",
       " 'Untitled31.ipynb',\n",
       " 'Untitled32.ipynb',\n",
       " 'Untitled33.ipynb',\n",
       " 'Untitled34.ipynb',\n",
       " 'Untitled35.ipynb',\n",
       " 'Untitled36.ipynb',\n",
       " 'Untitled4.ipynb',\n",
       " 'Untitled5.ipynb',\n",
       " 'Untitled6.ipynb',\n",
       " 'Untitled7.ipynb',\n",
       " 'Untitled8.ipynb',\n",
       " 'Untitled9.ipynb',\n",
       " 'varunnlp2.ipynb',\n",
       " 'Videos',\n",
       " 'VirtualBox VMs',\n",
       " 'Xilinx']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a217347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
